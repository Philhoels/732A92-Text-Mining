{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the packages needed\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import spacy\n",
    "from IPython.display import Image\n",
    "#from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for problem 5\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction (IE) is the task of **identifying named entities and semantic relations between these entities** in text data. In this lab we will focus on two sub-tasks in IE, **named entity recognition** (identifying mentions of entities) and **entity linking** (matching these mentions to entities in a knowledge base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we will be using has been tokenized following the conventions of the [Penn Treebank](ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html), and we need to prevent spaCy from using its own tokenizer on top of this. We therefore override spaCy&rsquo;s tokenizer with one that simply splits on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return Doc(self.vocab, words=text.split(\" \"))\n",
    "\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main **data set** for this lab is a **collection of news wire articles** in which **mentions of named entities** have been annotated with page names from the [English Wikipedia](https://en.wikipedia.org/wiki/). The next code cell loads the training and the development parts of the data into Pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with bz2.open(\"ner-train.tsv.bz2\", 'rt') as source:\n",
    "    df_train = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with bz2.open(\"ner-dev.tsv.bz2\", 'rt') as source:\n",
    "    df_dev = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in these two data frames corresponds to one mention of a named entity and has five columns:\n",
    "\n",
    "1. a unique identifier for the sentence containing the entity mention\n",
    "2. the pre-tokenized sentence, with tokens separated by spaces\n",
    "3. the start position of the token span containing the entity mention\n",
    "4. the end position of the token span (exclusive, as in Python list indexing)\n",
    "5. the entity label; either a **Wikipedia page name** or the **generic label** `--NME--`\n",
    "\n",
    "The following cell prints the first five samples from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000-001</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000-002</td>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brussels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                          sentence  beg  end  \\\n",
       "0    0000-000  EU rejects German call to boycott British lamb .    0    1   \n",
       "1    0000-000  EU rejects German call to boycott British lamb .    2    3   \n",
       "2    0000-000  EU rejects German call to boycott British lamb .    6    7   \n",
       "3    0000-001                                   Peter Blackburn    0    2   \n",
       "4    0000-002                               BRUSSELS 1996-08-22    0    1   \n",
       "\n",
       "            label  \n",
       "0         --NME--  \n",
       "1         Germany  \n",
       "2  United_Kingdom  \n",
       "3         --NME--  \n",
       "4        Brussels  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we see that the first sentence is annotated with three entity mentions:\n",
    "\n",
    "* the span 0–1 &lsquo;EU&rsquo; is annotated as a mention but only labelled with the generic `--NME--`\n",
    "* the span 2–3 &lsquo;German&rsquo; is annotated with the page [Germany](http://en.wikipedia.org/wiki/Germany)\n",
    "* the span 6–7 &lsquo;British&rsquo; is annotated with the page [United_Kingdom](http://en.wikipedia.org/wiki/United_Kingdom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU rejects German call to boycott British lamb .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the whole sentence with the id 0000-000\n",
    "df_train.loc[0][\"sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To warm up, we ask you to write code to **print** the **three measures** that you will be using **for evaluation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'precisionANDrecall.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c360fb0059ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"precisionANDrecall.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0;32m-> 1204\u001b[0;31m                 metadata=metadata)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'precisionANDrecall.png'"
     ]
    }
   ],
   "source": [
    "Image(filename = \"precisionANDrecall.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"F1score.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    \"\"\"Print precision, recall, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        Nothing, but prints the precision, recall, and F1 values computed\n",
    "        based on the specified sets.\n",
    "    \"\"\"\n",
    "    # TODO: Replace the next line with your own code\n",
    "    \n",
    "    # Compute precision\n",
    "    precision = len(gold.intersection(pred))/len(pred) * 100 # * 100 for percentage\n",
    "    # Compute recall\n",
    "    recall = len(pred.intersection(gold))/len(gold) * 100 # * 100 for percentage\n",
    "    # Compute F1 score\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"precision score: {}%\".format(round(precision,2)))\n",
    "    print(\"recall score: {}%\".format(round(recall,2)))\n",
    "    print(\"F1 score: {}%\".format(round(F1,2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report(set(range(3)), set(range(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a precision of 60%, a recall of 100%, and an F1-value of 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the **first tasks** that an information extraction system has to solve is to **locate and classify (mentions of) named entities**, such as *persons and organizations*. Here we will tackle the simpler task of **recognizing spans of tokens** that contain an entity mention, without the actual entity label.\n",
    "\n",
    "The English language model in spaCy features a full-fledged [named entity recognizer](https://spacy.io/usage/linguistic-features#named-entities) that identifies a variety of entities, and can be updated with new entity types by the user. Your task in this problem is to **evaluate the performance** of this component when **predicting entity spans** in the development data.\n",
    "\n",
    "Start by implementing a generator function that ***yields* the gold-standard spans** in a given data frame.\n",
    "\n",
    "**Hint:** The Pandas method [`itertuples()`](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.itertuples.html) is useful when iterating over the rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_spans(df):\n",
    "    \"\"\"Yield the gold-standard mention spans in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Replace the next line with your own code\n",
    "    for row in df_dev.itertuples():\n",
    "        yield row.sentence_id, row.beg, row.end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can count the spans yielded by your function. When called on the development data, you should get a **total** of **5,917** unique triples. The **first triple and the last triple** should be\n",
    "\n",
    "    ('0946-000', 2, 3)\n",
    "    ('1161-010', 1, 3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total of unique triples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the total length\n",
    "\n",
    "# use the gold_spans function and save as set\n",
    "spans_dev_gold = set(gold_spans(df_dev))\n",
    "print(\"Total of {} unique triples.\".format(len(spans_dev_gold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first triple and the last triple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the order of the triples\n",
    "# run a for loop and print the 1 and the last element\n",
    "for i, item in enumerate(spans_dev_gold):\n",
    "    if i == 0:\n",
    "        print(item)\n",
    "    if i == (len(spans_dev_gold)-1):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This is not the same order as expected. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to write code that **calls spaCy to predict the named entities in the development data**, and to **evaluate the accuracy of these predictions** in terms of precision, recall, and F1. **Print** these **scores** using the function that you wrote for Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to run and evaluate the spaCy NER on the development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob2_func(df):\n",
    "    # want to make predictions\n",
    "    # therefore we want to use spacys named Entity Recognition \n",
    "    # this we do by nlp() of a sentence, \n",
    "    # we need to access each sentence of the df (df_dev)\n",
    "    \n",
    "    # loop over each sentence by its index\n",
    "    for sentence_index in df.sentence_id.unique():\n",
    "        \n",
    "        # match the index of the loop with the index of the data frame and take its sentence\n",
    "        # - helpful link, how to work with iloc & loc - link below\n",
    "        #print(df.loc[df.sentence_id == sentence_index, \"sentence\"])\n",
    "        sentence = df.loc[df.sentence_id == sentence_index, \"sentence\"]\n",
    "        sentence = sentence.values[0] # 0 - access the sentence\n",
    "    \n",
    "        # named Entity Recognition, we want the start and the end of entity \n",
    "        doc = nlp(sentence) \n",
    "        for ent in doc.ents:\n",
    "            #print(sentence_index, ent.start, ent.end)\n",
    "            yield sentence_index, ent.start, ent.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction for problem 2 & save as set\n",
    "pred_prob2 = set(prob2_func(df_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report(spans_dev_gold, pred_prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you were able to see in **Problem&nbsp;2**, the **span accuracy** of the named entity recognizer is **far from perfect**. In particular, only slightly more than half of the predicted spans are correct according to the gold standard. Your next task is to **analyse** this **result in more detail**.\n",
    "\n",
    "**Write code** that **prints the false positives and the false negatives** from the **automatic prediction**. Have a look at the output. What are your **observations**? How could you **improve the result**? **Discuss** these questions in a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to do your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Print false positives and the false negatives of the prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename = \"FalsePositive_FalseNegative.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(gold, pred, nr_print, False_Positive = True):\n",
    "    if False_Positive == True:\n",
    "        # ---- Case: false positive\n",
    "        print(\"Case: false positive\")\n",
    "        difference = pred.difference(gold)\n",
    "    else:\n",
    "        # ---- Case: false negative\n",
    "        print(\"Case: false negative\")\n",
    "        difference = gold.difference(pred)\n",
    "        \n",
    "    # init the values I want to print later on\n",
    "    difference_list = []\n",
    "    difference_list_entity = []\n",
    "    difference_list_both = []\n",
    "\n",
    "    for item in difference:\n",
    "        #print(item)\n",
    "        # take the sentence of the sentence_id, can have multiple rows with the same sentence\n",
    "        sentence = df_dev.loc[df_dev.sentence_id == item[0], [\"sentence\"]]\n",
    "        # therefore, take the first value of the dataframe \n",
    "        sentence = sentence.iloc[0]\n",
    "        # converte to string and split to access specific word\n",
    "        sentence = sentence.values[0].split()\n",
    "        word = sentence[item[1]:item[2]]\n",
    "        # save specific word of specific item\n",
    "        #difference_list.append(word)\n",
    "\n",
    "        #------ get entitiy\n",
    "        # converte list to string\n",
    "        word_string = ' '.join(map(str, word))\n",
    "        # get the entitiy lable of the object\n",
    "        doc = nlp(word_string)\n",
    "        for ent in doc.ents:\n",
    "            difference_list_both.append([word_string,ent.label_])\n",
    "            difference_list_entity.append(ent.label_)\n",
    "\n",
    "    # ------------------- print the results -------------------\n",
    "    # print the word with its entity\n",
    "    print(\"The first {} word entitiy pairs:\".format(nr_print))\n",
    "    print(difference_list_both[0:nr_print])\n",
    "\n",
    "    # print a count of the entities\n",
    "    entity_counts = pd.Series(difference_list_entity)\n",
    "    all_counts = len(entity_counts)\n",
    "    entity_counts = entity_counts.value_counts()\n",
    "    print(\"Entity types count:\")\n",
    "    print(entity_counts)\n",
    "    print(\"Sum: {}\".format(all_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# error analysis - case: false positive\n",
    "error_analysis(spans_dev_gold,pred_prob2,10, False_Positive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis - case: false negative\n",
    "error_analysis(spans_dev_gold,pred_prob2,10, False_Positive = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Interpretation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Case: false positive**\n",
    "\n",
    "It can be seen that especially often numbers like the category Cardinal & date were wrongly classified.\n",
    "These are by far the most common mistakes. \n",
    "\n",
    "- **Case: false negative**\n",
    "\n",
    "Here it can be seen that organizations and individuals are generated the most misclassifications, as well as countries. \n",
    "\n",
    "However, these have a similar number of errors to the false positive case. Overall, the false positive case has a much higher number of errors than the false negative case. \n",
    "\n",
    "To improve the score, a similar procedure as in problem 3 could be performed, but without the entity types with the most errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the insights from your error analysis to improve the automated prediction that you implemented in Problem&nbsp;2. While the best way to do this would be to [update spaCy&rsquo;s NER model](https://spacy.io/usage/linguistic-features#updating) using domain-specific training data, for this lab it suffices to **write code to pre-process the data and/or post-process the output produced by spaCy**. You should be able to **improve** the **F1 score from Problem&nbsp;2 by at last 15 percentage points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to improve the span prediction from Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code as in prob2_func + light modifications\n",
    "def prob3_func(df):\n",
    "        for indx in df.sentence_id.unique():\n",
    "            sentence = df.loc[df.sentence_id==indx,'sentence']\n",
    "            doc = nlp(sentence.values[0])\n",
    "            for ent in doc.ents:\n",
    "                # modification at this part\n",
    "                # exclude some specific Named Entity Recognition\n",
    "                # ---- Test combinations\n",
    "                #if ent.label_ not in ['CARDINAL', 'DATE', 'ORG', 'Person', 'GPE']:\n",
    "                #if ent.label_ not in ['CARDINAL', 'DATE', 'ORG', 'PERSON', 'ORDINAL', 'GPE', 'TIME', 'QUANTITY','PERCENT','MONEY','NORP', 'EVENT']:\n",
    "                # ----\n",
    "                if ent.label_ not in ['CARDINAL', 'ORDINAL', 'QUANTITY', 'MONEY', 'PERCENT', 'TIME', 'DATE']:\n",
    "                    yield indx,ent.start,ent.end\n",
    "# make predictions on the same dataset with the new function                   \n",
    "new_preds = set(prob3_func(df_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show** that you achieve the **performance goal** by reporting the evaluation measures that you implemented in Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_report(spans_dev_gold,new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The F1 score improved by {} %.\".format(round(75.89-59.9,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, we ask you to **store** the **outputs of the improved named entity recognizer on the development data in a new data frame**. This **new frame** should have the **same layout as the original data frame for the development data** that you loaded above, but should contain the predicted start and end positions for each token span. As the `label` of **each span**, you can use the **special value** `--NME--`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to store the predicted spans in a new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction as list\n",
    "preds_list = list(new_preds)\n",
    "\n",
    "x=[item[0] for item in preds_list]\n",
    "y=[item[1] for item in preds_list]\n",
    "z=[item[2] for item in preds_list]\n",
    "\n",
    "# create a list with the indexes for each predicted span\n",
    "indexes_list = []\n",
    "for i in range(len(x)):\n",
    "    indexes_list.append(df_dev.index[(df_dev['sentence_id']==x[i]) & (df_dev['beg']==y[i]) & (df_dev['end']==z[i])].tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "# flatten the indexes list \n",
    "flat_list = reduce(lambda x, y: x+y, indexes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "df_dev_small = df_dev.iloc[flat_list,:]\n",
    "df_dev_small.drop(['label'],axis=1)\n",
    "df_dev_small.label='--NME--'\n",
    "df_dev_small.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method for predicting mention spans, we turn to the task of **entity linking**, which amounts to predicting the knowledge base entity that is referenced by a given mention. In our case, **for each span we want to predict the Wikipedia page that this mention references**.\n",
    "\n",
    "Start by **extending** the generator function that you implemented in **Problem&nbsp;2** to labelled spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_mentions(df):\n",
    "    \"\"\"Yield the gold-standard mentions in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and entity label of each span.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Replace the next line with your own code\n",
    "    # code from Probem 2 - gold_spans\n",
    "    for row in df_dev.itertuples():\n",
    "        # added lables\n",
    "        yield row.sentence_id, row.beg, row.end, row.label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **naive baseline for entity linking** on our data set is to **link each mention span to the Wikipedia page name** that we get when we ***join* the tokens in the span by underscores**, as is standard in Wikipedia page names. Suppose, **for example**, that a span contains the two tokens\n",
    "\n",
    "    Jimi Hendrix\n",
    "\n",
    "The baseline Wikipedia page name for this span would be\n",
    "\n",
    "    Jimi_Hendrix\n",
    "\n",
    "**Implement** this **naive baseline and evaluate its performance**. Print the evaluation measures that you implemented in Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Here and in the remainder of this lab, you should base your entity predictions on the predicted spans that you computed in Problem&nbsp;3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dev_small.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create predictions\n",
    "def prob4_func(df):\n",
    "    # run every row of the df\n",
    "    for row in range(df_dev_small.shape[0]):\n",
    "        # get the sentence_id \n",
    "        sentence_id = df_dev_small.iloc[row,0]\n",
    "        \n",
    "        # get the beg and the end of specific word\n",
    "        beg = df_dev_small.iloc[row,2]\n",
    "        end = df_dev_small.iloc[row,3]\n",
    "        \n",
    "        # access every word of the sentence\n",
    "        sentence = df_dev_small.iloc[row,1].split()\n",
    "        # filter for word(s) we want to have\n",
    "        word = sentence[beg:end]\n",
    "        \n",
    "        # join the words with \"_\"\n",
    "        words = \"_\".join(word)\n",
    "        \n",
    "        yield sentence_id, beg, end, words\n",
    "# make predictions \n",
    "preds4 = set(prob4_func(df_dev_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gold_mentions function and save as set\n",
    "gold_mentions_set = set(gold_mentions(df_dev_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluation_report(gold_mentions_set,preds4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Extending the training data using the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art approaches to **entity linking exploit information in knowledge bases**. In our case, where **Wikipedia is the knowledge base**, one particularly useful type of information are links to other Wikipedia pages. In particular, we can **interpret the anchor texts** (the highlighted texts that you click on when you click on a link) as **mentions of the entities** (pages) that they link to. This allows us to harvest long lists over mention–entity pairings.\n",
    "\n",
    "The following cell loads a data frame summarizing anchor texts and page references harvested from the first paragraphs of the English Wikipedia. The data frame also contains all entity mentions in the training data (but not the development or the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"kb.tsv.bz2\", 'rt') as source:\n",
    "    df_kb = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what information is available in this data, the following cell shows the entry for the anchor text `Sweden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kb.loc[df_kb[\"mention\"] == \"Sweden\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, **each row** of the data frame **contains a pair $(m, e)$ of a mention $m$ and an entity $e$**, as well as the **conditional probability $P(e|m)$ for mention $m$ referring to entity $e$**. These probabilities were **estimated based on the frequencies of mention–entity pairs in the knowledge base**. The **example** shows that the **anchor text &lsquo;Sweden&rsquo;** is most often used to refer to the entity [Sweden](http://en.wikipedia.org/wiki/Sweden), but in a few cases also to refer to Sweden&rsquo;s national football and ice hockey teams. Note that references are sorted in decreasing order of probability, so that the **most probable pairing stands first**.\n",
    "\n",
    "**Implement** an **entity linking method** that **resolves each mention to the most probable entity in the data frame**. If the mention is not included in the data frame, you can predict the generic label `--NME--`. **Print the precision, recall, and F1** of your method using the function that you implemented for Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the \"most probable entity\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_kb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create predictions\n",
    "def prob5_func(df):\n",
    "    # run every row of the df\n",
    "    for row in range(df.shape[0]):\n",
    "        # get the mention \n",
    "        mention = df.iloc[row,0]\n",
    "        \n",
    "        # get entity with highest prob\n",
    "        mention_df = df.loc[df[\"mention\"] == mention]\n",
    "        \n",
    "        ### ------ flag ------\n",
    "        # if df not empty -> take highest prob\n",
    "        # else -- NME --\n",
    "        if mention_df.shape[0] != 0:\n",
    "            mention_df_highest_entity = mention_df.iloc[0,1]\n",
    "            yield mention, mention_df_highest_entity\n",
    "        else:\n",
    "            yield mention, \"--NME--\" \n",
    "# make predictions\n",
    "preds5 = set(prob5_func(df_kb))\n",
    "#print(preds5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We also need a gold-standard to evaluate the results\n",
    "def gold_mention_entitiy(df):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    # code from Probem 2 - gold_spans\n",
    "    for row in df.itertuples():\n",
    "        # added lables\n",
    "        yield row.mention, row.entity\n",
    "\n",
    "# create gold standard\n",
    "gold_mention_entity_set = set(gold_mention_entitiy(df_kb))\n",
    "#print(gold_mention_entity_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluation_report(gold_mention_entity_set,preds5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Question 1 - Problem 5: Results accuracy </strong> \n",
    "\n",
    "> Not sure if I did understand the question 100% correctly.\n",
    "The accuracy seems strange to me, especially since I read problem 6. Not sure if the results should be that good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Context-sensitive disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the **entity mention &lsquo;Lincoln&rsquo;**. The **most probable entity** for this mention turns out to be [Lincoln, Nebraska](http://en.wikipedia.org/Lincoln,_Nebraska); but in pages about American history, we would be **better off to predict** [Abraham Lincoln](http://en.wikipedia.org/Abraham_Lincoln). This suggests that we should **try to disambiguate between different entity references based on the textual context on the page from which the mention was taken**. Your task in this last problem is to implement this idea.\n",
    "\n",
    "**Set up** a **dictionary** that **contains**, for each **mention $m$ that can refer to more than one entity $e$**, a **separate Naive Bayes classifier to predict the correct entity $e$**, given the textual context of the mention. As the prior probabilities of the classifier, **choose** the **probabilities $P(e|m)$** that you used in Problem&nbsp;5. To let you **estimate the context-specific probabilities**, we have compiled a data set with mention contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"contexts.tsv.bz2\") as source:\n",
    "    df_contexts = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **data frame contains**, for each **ambiguous mention $m$ and each knowledge base entity $e$ to which this mention can refer**, up to 100 randomly selected contexts in which $m$ is used to refer to $e$. For this data, a **context** is **defined as a bag of words** containing the mention itself, as well as the **5 tokens to the left and the 5 tokens to the right** of the mention. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contexts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data frame, it is easy to select the **data that you need to train the classifiers** – the **contexts** and corresponding **entities for all mentions**. To illustrate this, the following cell **shows how to select all contexts that belong to the mention &lsquo;Lincoln&rsquo;**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contexts.context[df_contexts.mention == \"Lincoln\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the context-sensitive disambiguation method and evaluate its performance**. Here are some more hints that may help you along the way:\n",
    "\n",
    "**Hint 1:** The **prior probabilities** for a Naive Bayes classifier can be **specified using** the `class_prior` option. You will have to provide the **probabilities** in the same **order as the alphabetically sorted class (entity) names.**\n",
    "\n",
    "**Hint 2:** To **tune** the **performance of your method**, you can try to **tweak the behaviour of the vectorizer** (for example, should it apply lowercasing or not?) and the **width of the window** from which you are extracting context tokens at prediction time.\n",
    "\n",
    "**Hint 3:** Not all mentions in the knowledge base are ambiguous, and therefore not all mentions have context data. If a mention has only **one possible entity, pick that one. If a mention has no entity at all, predict the `--NME--` label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the context-sensitive disambiguation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test - Start**\n",
    "\n",
    "This part can be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Test for piror probs & fit **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_contexts.mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_contexts.mention))\n",
    "print(len(df_contexts.mention.unique()))\n",
    "print(\"There are {} mention which are more often in the df.\".format(len(df_contexts.mention) - len(df_contexts.mention.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each mention m, just take the unique mentions\n",
    "for i in df_contexts.mention.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_contexts.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict test\n",
    "dict = {\n",
    "    \"Phillip\": [\"A\", \"B\"],\n",
    "    \"Marie\": \"C\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict[\"Phillip\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill dict\n",
    "dict[\"Rosa\"] = [\"D\", \"E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict[\"Phillip\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill a dict in a loop, generate test data set for this\n",
    "test_df = df_contexts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.entity[test_df.mention == \"1970\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "for mention in test_df.mention.unique():\n",
    "    test_dict[mention] = test_df.entity[test_df.mention == mention].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dict)\n",
    "# number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of each mention and all correspondingly entities\n",
    "dict_m_e = {}\n",
    "for mention in df_contexts.mention.unique():\n",
    "    dict_m_e[mention] = df_contexts.entity[df_contexts.mention == mention].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of mentions, should be 1245 (tested before)\n",
    "len(dict_m_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_m_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the probs from the mention test above\n",
    "df_kb.loc[df_kb.mention == \"1970\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_kb.loc[df_kb.mention == \"1990 World Cup\"]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(by = [\"entity\"]).prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same order as the alphabetically sorted class (entity) names.\n",
    "df_test.sort_values(by = [\"entity\"]).prob.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mention in df_contexts.mention.unique():\n",
    "    class_prob_df = df_kb.loc[df_kb.mention == mention]\n",
    "    class_prob_test = class_prob_df.sort_values(by = [\"entity\"]).prob.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob_test\n",
    "# saves the prob for each mention\n",
    "# run the pipline in loop & input this vector for each iteration!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline test\n",
    "Pipeline_CV_MNB_test = Pipeline([\n",
    "    ('vect', CountVectorizer(lowercase=False)), # default = True\n",
    "    ('clf', MultinomialNB(class_prior = class_prob_test)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit pipeline with X (mention) & y (entity)\n",
    "pipefit_test = Pipeline_CV_MNB_test.fit(X = df_contexts.context[df_contexts.mention == mention],\n",
    "                                   y = df_contexts.entity[df_contexts.mention == mention]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipefit_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Test for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = df_contexts[df_contexts.mention == mention]\n",
    "test_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_context = test_context.context.tolist()[0]\n",
    "print(test_context)\n",
    "print(type(test_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipefit_test.predict([test_context])\n",
    "# nice that looks good!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipefit_test.predict([test_context])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dev_small.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dev_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_small.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to make predictions from df of problem 3\n",
    "# we gonna label the --NME-- of df_dev_small\n",
    "\n",
    "# we have to access the mention for each sentence in the df_dev_small\n",
    "row = df_dev_small.shape[0] -1\n",
    "sentence_test = df_dev_small.iloc[row,1].split()\n",
    "print(sentence_test)\n",
    "beg = df_dev_small.iloc[row,2]\n",
    "end = df_dev_small.iloc[row,3]\n",
    "mention_test = sentence_test[beg:end]\n",
    "print(mention_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to access the context, which is needed to make the prediction of the mention\n",
    "# 5 tokens to the left and the 5 tokens to the right of the mention\n",
    "context_test = sentence_test[max(0,(beg-5)):min((end+5), len(sentence_test))]\n",
    "context_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\" \".join(context_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min(9,len(context_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(mention_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to check, later check if mention exists\n",
    "mention_df_test = df_contexts[df_contexts.mention == \" \".join(mention_test)]\n",
    "mention_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that the data is empty, for the frame\n",
    "mention_df_test.shape[0]\n",
    "# if it´s 0, I can´t make any prediction!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mention_test_string = \" \".join(mention_test)\n",
    "context_test_string = \" \".join(context_test)\n",
    "dict_fit[mention_test_string].predict([context_test_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fit[mention_test_string].predict([context_test_string])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test - End**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate Naive Bayes classifier to predict the correct entity\n",
    "# given the textual context of the mention\n",
    "\n",
    "# init the dictionary\n",
    "dict_fit = {}\n",
    "\n",
    "for mention in df_contexts.mention.unique():\n",
    "    \n",
    "    # set up the pipeline\n",
    "\n",
    "    # we have to the input for MultinomialNB(class_prior= input)\n",
    "    # use the prob from problem 5\n",
    "\n",
    "    # get class_probs\n",
    "    class_prob_df = df_kb.loc[df_kb.mention == mention]\n",
    "    class_prob = class_prob_df.sort_values(by = [\"entity\"]).prob.tolist() # same order as the alphabetically sorted class (entity) names.\n",
    "\n",
    "    # create pipeline\n",
    "    Pipeline_CV_MNB = Pipeline([\n",
    "        ('vect', CountVectorizer(lowercase=False)), # default = True\n",
    "        ('clf', MultinomialNB(class_prior = class_prob)),\n",
    "    ])\n",
    "    \n",
    "    # fit every mention\n",
    "    # fit the pipeline with X (mention) & y (entity)\n",
    "    fit = Pipeline_CV_MNB.fit(X = df_contexts.context[df_contexts.mention == mention],\n",
    "                              y = df_contexts.entity[df_contexts.mention == mention])\n",
    "    \n",
    "    dict_fit[mention] = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create predictions\n",
    "def prob6_func(df):\n",
    "    # run every row of the df\n",
    "    for row in range(df.shape[0]):\n",
    "        # want to make predictions from df of problem 3\n",
    "        # we gonna label the --NME-- of df_dev_small\n",
    "\n",
    "        # we have to access the mention for each sentence in the df_dev_small\n",
    "        sentence = df.iloc[row,1].split()\n",
    "\n",
    "        # we need the beginning and the end to access the mention\n",
    "        beg = df.iloc[row,2]\n",
    "        end = df.iloc[row,3]\n",
    "        mention = sentence[beg:end]\n",
    "\n",
    "        # we also need to access the context, which is needed to make the prediction of the mention\n",
    "        # 5 tokens to the left and the 5 tokens to the right of the mention\n",
    "        context = sentence_test[max(0,(beg-5)):min((end+5), len(sentence))]\n",
    "        # need the context as string for the prediction\n",
    "        context_string = \" \".join(context)\n",
    "\n",
    "        ### ------ flag ------\n",
    "\n",
    "        # create df, later check if mention exists\n",
    "        mention_test_string = \" \".join(mention)\n",
    "        mention_df = df_contexts[df_contexts.mention == mention_test_string]\n",
    "\n",
    "        # if df not empty -> take highest prob\n",
    "        # else -- NME --\n",
    "        if mention_df.shape[0] != 0:\n",
    "            # make predictions\n",
    "            label = dict_fit[mention_test_string].predict([context_string])[0]\n",
    "            yield mention_test_string, label\n",
    "        else:\n",
    "            yield mention_test_string, \"--NME--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds6 = set(prob6_func(df_dev_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluation_report(gold_mention_entity_set,preds6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should **expect** to see a **small (around 1&nbsp;unit) increase** in both precision, recall, and F1. Published systems report a larger impact of context-sensitive disambiguation. Feel free to think about what could explain the relatively minor impact that we see here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Question 2 - Problem 6: Results accuracy </strong> \n",
    "\n",
    "> In problem 5 I had the problem that the accuracy of my opinion was very high, too high to be correct! \n",
    "In problem 6 I have the opposite problem. The accuracy is so bad that I can hardly imagine that this is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This was the last lab in the Text Mining course. Well done!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Please read the section ‘General information’ on the ‘Labs’ page of the course website before submitting this notebook!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful links**\n",
    "\n",
    "- Problem 1\n",
    "\n",
    "https://www.geeksforgeeks.org/intersection-function-python/\n",
    "\n",
    "- Problem 2\n",
    "\n",
    "https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/\n",
    "\n",
    "https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n",
    "\n",
    "- Problem 3\n",
    "\n",
    "https://dev.to/svinci/intersection-union-and-difference-of-sets-in-python-4gkn\n",
    "\n",
    "- Problem 6\n",
    "\n",
    "https://www.w3schools.com/python/python_dictionaries.asp\n",
    "\n",
    "https://stackoverflow.com/questions/30280856/populating-a-dictionary-using-for-loops-python\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
